{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T18:52:17.923008Z",
     "start_time": "2019-07-29T18:52:17.912883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "utils.load_extension('collapsible_headings/main')\n",
       "utils.load_extension('hide_input/main')\n",
       "utils.load_extension('autosavetime/main')\n",
       "utils.load_extension('execute_time/ExecuteTime')\n",
       "utils.load_extension('table_beautifier/main')\n",
       "utils.load_extension('code_prettify/code_prettify')\n",
       "utils.load_extension('scroll_down/main')\n",
       "utils.load_extension('jupyter-js-widgets/extension')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "utils.load_extension('collapsible_headings/main')\n",
    "utils.load_extension('hide_input/main')\n",
    "utils.load_extension('autosavetime/main')\n",
    "utils.load_extension('execute_time/ExecuteTime')\n",
    "utils.load_extension('table_beautifier/main')\n",
    "utils.load_extension('code_prettify/code_prettify')\n",
    "utils.load_extension('scroll_down/main')\n",
    "utils.load_extension('jupyter-js-widgets/extension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T18:54:41.988285Z",
     "start_time": "2019-07-29T18:54:41.956511Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch,math,sys\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from functools import partial\n",
    "from fastai.torch_core import Module\n",
    "import math\n",
    "\n",
    "__all__ = ['XResNet_1d', 'xresnet18_1d', 'xresnet34_1d', 'xresnet50_1d', 'xresnet101_1d', 'xresnet152_1d']\n",
    "\n",
    "# or: ELU+init (a=0.54; gain=1.55)\n",
    "act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "def init_cnn(m):\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv1d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "        \n",
    "def same_padding1d(seq_len, ks, stride=1, dilation=1):\n",
    "    pad = (stride * (seq_len - 1) - seq_len + ks + (ks - 1) * (dilation - 1)) / 2\n",
    "    padding = (math.ceil(pad), int(pad))\n",
    "    return padding\n",
    "\n",
    "def conv(ni, nf, seq_len, ks=3, stride=1, padding='same', bias=False):\n",
    "    if padding == 'same': padding = same_padding1d(seq_len, ks, stride=stride)\n",
    "    else: padding = ks//2\n",
    "    return nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "def noop(x): return x\n",
    "\n",
    "def conv_layer(ni, nf, seq_len, ks=3, stride=1, padding='same', zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm1d(nf)\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    layers = [conv(ni, nf, seq_len, ks, stride=stride, padding=padding), bn]\n",
    "    if act: \n",
    "        layers.append(act_fn)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResBlock(Module):\n",
    "    def __init__(self, expansion, ni, nh, seq_len, padding='same', stride=1):\n",
    "        nf,ni = nh*expansion,ni*expansion\n",
    "        if expansion == 1:\n",
    "            self.conv_layer1 = conv_layer(ni, nh, seq_len, 3, stride=stride, padding=padding)\n",
    "            self.conv_layer2 = conv_layer(nh, nf, seq_len, 3, zero_bn=True, act=False, padding=padding)\n",
    "            self.conv_layer3 = noop\n",
    "        else:\n",
    "            self.conv_layer1 = conv_layer(ni, nh, seq_len, 1, padding=padding)\n",
    "            self.conv_layer2 = conv_layer(nh, nh, seq_len, 3, stride=stride, padding=padding)\n",
    "            self.conv_layer3 = conv_layer(nh, nf, seq_len, 1, zero_bn=True, act=False, padding=padding)\n",
    "        # TODO: check whether act=True works better\n",
    "        self.idconv = noop if ni==nf else conv_layer(ni, nf, seq_len, 1, act=False, padding=padding)\n",
    "        self.pool = noop if stride==1 else nn.AvgPool1d(2, ceil_mode=True)\n",
    "\n",
    "    def forward(self, x): \n",
    "        print('x:', x.shape)\n",
    "        out = self.conv_layer1(x)\n",
    "        print('out1:', out.shape)\n",
    "        out = self.conv_layer2(x)\n",
    "        print('out2:', out.shape)\n",
    "        out = self.conv_layer3(x)\n",
    "        print('out3:', out.shape)\n",
    "                   \n",
    "        convid = self.idconv(self.pool(x))\n",
    "        print('convid:', convid.shape)\n",
    "        output = act_fn(out + convid)\n",
    "        print('output:', output.shape)\n",
    "        return output\n",
    "\n",
    "class XResNet_1d(nn.Sequential):\n",
    "    def __init__(self, expansion, layers, c_in, seq_len, c_out=1000, padding='same'):\n",
    "        stem = []\n",
    "        sizes = [c_in,32,32,64]\n",
    "        for i in range(3):\n",
    "            stem.append(conv_layer(sizes[i], sizes[i+1], seq_len, padding=padding, stride=2 if i==0 else 1))\n",
    "\n",
    "        block_szs = [64//expansion,64,128,256,512]\n",
    "        blocks = [self._make_layer(expansion, block_szs[i], block_szs[i+1], l, 1 if i==0 else 2, padding=padding)\n",
    "                  for i,l in enumerate(layers)]\n",
    "        super().__init__(\n",
    "            *stem,\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
    "            *blocks,\n",
    "            nn.AdaptiveAvgPool1d(1), Flatten(),\n",
    "            nn.Linear(block_szs[-1]*expansion, c_out),\n",
    "        )\n",
    "        init_cnn(self)\n",
    "\n",
    "    def _make_layer(self, expansion, ni, nf, blocks, stride, padding='same'):\n",
    "        return nn.Sequential(\n",
    "            *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1, padding=padding)\n",
    "              for i in range(blocks)])\n",
    "\n",
    "def xresnet(expansion, n_layers, name, pretrained=False, **kwargs):\n",
    "    model = XResNet_1d(expansion, n_layers, **kwargs)\n",
    "    if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[name]))\n",
    "    return model\n",
    "\n",
    "me = sys.modules[__name__]\n",
    "for n, e, l in [\n",
    "    [18, 1, [2, 2, 2, 2]],\n",
    "    [34, 1, [3, 4, 6, 3]],\n",
    "    [50, 4, [3, 4, 6, 3]],\n",
    "    [101, 4, [3, 4, 23, 3]],\n",
    "    [152, 4, [3, 8, 36, 3]],\n",
    "]:\n",
    "    name = f'xresnet{n}_1d'\n",
    "    setattr(me, name, partial(xresnet, expansion=e, n_layers=l, name=name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "#export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch,math,sys\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from functools import partial\n",
    "from fastai.torch_core import Module\n",
    "\n",
    "__all__ = ['XResNet', 'xresnet18', 'xresnet34', 'xresnet50', 'xresnet101', 'xresnet152']\n",
    "\n",
    "# or: ELU+init (a=0.54; gain=1.55)\n",
    "act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "def init_cnn(m):\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "\n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n",
    "\n",
    "def noop(x): return x\n",
    "\n",
    "def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm2d(nf)\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    layers = [conv(ni, nf, ks, stride=stride), bn]\n",
    "    if act: \n",
    "        layers.append(act_fn)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResBlock(Module):\n",
    "    def __init__(self, expansion, ni, nh, stride=1):\n",
    "        nf,ni = nh*expansion,ni*expansion\n",
    "        layers  = [conv_layer(ni, nh, 3, stride=stride),\n",
    "                   conv_layer(nh, nf, 3, zero_bn=True, act=False)\n",
    "        ] if expansion == 1 else [\n",
    "                   conv_layer(ni, nh, 1),\n",
    "                   conv_layer(nh, nh, 3, stride=stride),\n",
    "                   conv_layer(nh, nf, 1, zero_bn=True, act=False)\n",
    "        ]\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "        # TODO: check whether act=True works better\n",
    "        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False)\n",
    "        self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n",
    "        #self.bn = nn.BatchNorm2d(nf)\n",
    "\n",
    "    def forward(self, x): return act_fn(self.convs(x) + self.idconv(self.pool(x)))\n",
    "    #def forward(self, x): return self.bn(act_fn(self.convs(x) + self.idconv(self.pool(x))))\n",
    "\n",
    "def filt_sz(recep): return min(64, 2**math.floor(math.log2(recep*0.75)))\n",
    "\n",
    "class XResNet(nn.Sequential):\n",
    "    def __init__(self, expansion, layers, c_in=3, c_out=1000):\n",
    "        stem = []\n",
    "        sizes = [c_in,32,32,64]\n",
    "        for i in range(3):\n",
    "            stem.append(conv_layer(sizes[i], sizes[i+1], stride=2 if i==0 else 1))\n",
    "            #nf = filt_sz(c_in*9)\n",
    "            #stem.append(conv_layer(c_in, nf, stride=2 if i==1 else 1))\n",
    "            #c_in = nf\n",
    "\n",
    "        block_szs = [64//expansion,64,128,256,512]\n",
    "        blocks = [self._make_layer(expansion, block_szs[i], block_szs[i+1], l, 1 if i==0 else 2)\n",
    "                  for i,l in enumerate(layers)]\n",
    "        super().__init__(\n",
    "            *stem,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            *blocks,\n",
    "            nn.AdaptiveAvgPool2d(1), \n",
    "            Flatten(),\n",
    "            nn.Linear(block_szs[-1]*expansion, c_out),\n",
    "        )\n",
    "        init_cnn(self)\n",
    "\n",
    "    def _make_layer(self, expansion, ni, nf, blocks, stride):\n",
    "        return nn.Sequential(\n",
    "            *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1)\n",
    "              for i in range(blocks)])\n",
    "\n",
    "def xresnet(expansion, n_layers, name, pretrained=False, **kwargs):\n",
    "    model = XResNet(expansion, n_layers, **kwargs)\n",
    "    if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[name]))\n",
    "    return model\n",
    "\n",
    "me = sys.modules[__name__]\n",
    "for n, e, l in [\n",
    "    [18, 1, [2, 2, 2, 2]],\n",
    "    [34, 1, [3, 4, 6, 3]],\n",
    "    [50, 4, [3, 4, 6, 3]],\n",
    "    [101, 4, [3, 4, 23, 3]],\n",
    "    [152, 4, [3, 8, 36, 3]],\n",
    "]:\n",
    "    name = f'xresnet{n}'\n",
    "    setattr(me, name, partial(xresnet, expansion=e, n_layers=l, name=name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T18:54:42.796765Z",
     "start_time": "2019-07-29T18:54:42.784941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "{\n",
       "    const ip = IPython.notebook\n",
       "    if (ip) {\n",
       "        ip.save_notebook()\n",
       "        console.log('a')\n",
       "        const s = `!python notebook2script.py ${ip.notebook_name}`\n",
       "        if (ip.kernel) { ip.kernel.execute(s) }\n",
       "    }\n",
       "    }"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "{\n",
       "    const ip = IPython.notebook\n",
       "    if (ip) {\n",
       "        ip.save_notebook()\n",
       "        console.log('a')\n",
       "        const s = `!python notebook2script.py ${ip.notebook_name}`\n",
       "        if (ip.kernel) { ip.kernel.execute(s) }\n",
       "    }\n",
       "    }"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "{\n",
       "    const ip = IPython.notebook\n",
       "    if (ip) {\n",
       "        ip.save_notebook()\n",
       "        console.log('a')\n",
       "        const s = `!python notebook2script.py ${ip.notebook_name}`\n",
       "        if (ip.kernel) { ip.kernel.execute(s) }\n",
       "    }\n",
       "    }"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "{\n",
       "    const ip = IPython.notebook\n",
       "    if (ip) {\n",
       "        ip.save_notebook()\n",
       "        console.log('a')\n",
       "        const s = `!python notebook2script.py ${ip.notebook_name}`\n",
       "        if (ip.kernel) { ip.kernel.execute(s) }\n",
       "    }\n",
       "    }"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "{\n",
       "    const ip = IPython.notebook\n",
       "    if (ip) {\n",
       "        ip.save_notebook()\n",
       "        console.log('a')\n",
       "        const s = `!python notebook2script.py ${ip.notebook_name}`\n",
       "        if (ip.kernel) { ip.kernel.execute(s) }\n",
       "    }\n",
       "    }"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try: from exp.nb_utils import *\n",
    "except ImportError: from .nb_utils import *\n",
    "nb_auto_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T18:54:44.362497Z",
     "start_time": "2019-07-29T18:54:43.907656Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected padding to be a single integer value or a list of 1 values to match the convolution dimensions, but got padding=[65, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-837ad0462210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mc_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mxresnet50_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai-v1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai-v1/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai-v1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai-v1/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai-v1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai-v1/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    194\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    195\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 196\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected padding to be a single integer value or a list of 1 values to match the convolution dimensions, but got padding=[65, 64]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "c_in = 10\n",
    "seq_len = 128\n",
    "c_out = 2\n",
    "xb = torch.rand(32, c_in, seq_len)\n",
    "xresnet50_1d(c_in=c_in, seq_len=seq_len, c_out=2)(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T18:02:20.776763Z",
     "start_time": "2019-07-29T18:02:20.414686Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xresnet50_1d(c_in=c_in, c_out=c_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T18:04:07.575702Z",
     "start_time": "2019-07-29T18:04:07.417311Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "__all__ = ['ResNetPlus', 'resnetplus']\n",
    "\n",
    "def init_cnn(m):\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children():\n",
    "        print(l.name)\n",
    "        init_cnn(l)\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16, se=True):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.se = se\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.se:\n",
    "            return x\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        out = x * y.expand_as(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AddCoords1d(nn.Module):\n",
    "    def __init__(self, coordconv=False):\n",
    "        super().__init__()\n",
    "        self.coordconv = coordconv\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.\n",
    "                                   is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        if not self.coordconv:\n",
    "            return input_tensor\n",
    "        batch_size, feat, sl = input_tensor.size()\n",
    "        if self.coordconv:\n",
    "            xx_channel = torch.arange(\n",
    "                sl, device=self.device, dtype=torch.float) / (sl - 1)\n",
    "            xx_channel = xx_channel * 2 - 1\n",
    "            xx_channel = xx_channel.repeat(batch_size, 1, 1)\n",
    "            input_tensor = torch.cat([input_tensor, xx_channel], dim=1)\n",
    "        return input_tensor\n",
    "\n",
    "\n",
    "def same_padding1d(input_dim, kernel_size, stride=1, dilation=1):\n",
    "    # input dim: seq_len\n",
    "    import math\n",
    "    pad = (stride * (input_dim - 1) - input_dim + kernel_size +\n",
    "           (kernel_size - 1) * (dilation - 1)) / 2\n",
    "    return (math.ceil(pad), int(pad))\n",
    "\n",
    "\n",
    "def conv1d(in_features, n_feature_maps, kernel_size, stride=1, padding=0, bias=False):\n",
    "    return nn.Conv1d(\n",
    "        in_channels=in_features,\n",
    "        out_channels=n_feature_maps,\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        stride=stride,\n",
    "        bias=bias)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 n_feature_maps,\n",
    "                 expand=True,\n",
    "                 se=True,\n",
    "                 reduction=16,\n",
    "                 coordconv=False,\n",
    "                 bias=False,\n",
    "                 device='cuda'):\n",
    "        super().__init__()\n",
    "        self.expand = expand\n",
    "\n",
    "        self.addcoords1d = AddCoords1d(coordconv=coordconv)\n",
    "        self.conv1 = conv1d(\n",
    "            in_features + coordconv, n_feature_maps, kernel_size=7, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm1d(n_feature_maps)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = conv1d(n_feature_maps, n_feature_maps, kernel_size=5, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm1d(n_feature_maps)\n",
    "\n",
    "        self.conv3 = conv1d(n_feature_maps, n_feature_maps, kernel_size=3, bias=bias)\n",
    "        self.bn3 = nn.BatchNorm1d(n_feature_maps)\n",
    "        self.se = SELayer(n_feature_maps, reduction, se)\n",
    "\n",
    "        # expand channels for the sum\n",
    "        self.shortcut_conv = conv1d(in_features, n_feature_maps, kernel_size=1, bias=bias)\n",
    "        self.shortcut_bn = nn.BatchNorm1d(n_feature_maps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.addcoords1d(x)\n",
    "        m = nn.ConstantPad1d(same_padding1d(x.shape[2], 7), 0)\n",
    "        x = m(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        m = nn.ConstantPad1d(same_padding1d(out.shape[2], 5), 0)\n",
    "        out = m(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        m = nn.ConstantPad1d(same_padding1d(out.shape[2], 3), 0)\n",
    "        out = m(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.se(out)\n",
    "\n",
    "        if self.expand:\n",
    "            shortcut = self.shortcut_conv(residual)\n",
    "            shortcut = self.shortcut_bn(shortcut)\n",
    "        else:\n",
    "            shortcut = self.shortcut_bn(residual)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetPlus(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 n_classes,\n",
    "                 input_dropout=0.0,\n",
    "                 fc_dropout=0.0,\n",
    "                 se=False,\n",
    "                 reduction=16,\n",
    "                 coordconv=False,\n",
    "                 y_range=None,\n",
    "                 bias=False,\n",
    "                 init=False):\n",
    "        super().__init__()\n",
    "        n_feature_maps = 64\n",
    "\n",
    "        self.arch = 'ResNet'\n",
    "        self.se = se\n",
    "        self.reduction = reduction\n",
    "        self.coordconv = coordconv\n",
    "        if self.se:\n",
    "            self.arch = 'SE' + self.arch\n",
    "        if self.coordconv:\n",
    "            self.arch = 'cc' + self.arch\n",
    "        if input_dropout != 0.0:\n",
    "            self.arch = self.arch + '_ido'\n",
    "        if fc_dropout != 0.0:\n",
    "            self.arch = self.arch + '_do'\n",
    "        self.__name__ = self.arch\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.block1 = ResBlock(\n",
    "            in_features,\n",
    "            n_feature_maps,\n",
    "            se=se,\n",
    "            reduction=reduction,\n",
    "            coordconv=coordconv, \n",
    "            bias=bias)\n",
    "        self.block2 = ResBlock(\n",
    "            n_feature_maps,\n",
    "            n_feature_maps * 2,\n",
    "            se=se,\n",
    "            reduction=reduction, \n",
    "            bias=bias)\n",
    "        self.block3 = ResBlock(\n",
    "            n_feature_maps * 2,\n",
    "            n_feature_maps * 2,\n",
    "            expand=False,\n",
    "            se=se,\n",
    "            reduction=reduction, \n",
    "            bias=bias)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "        self.fc1 = nn.Linear(n_feature_maps * 2, n_classes)\n",
    "\n",
    "        self.y_range = y_range\n",
    "\n",
    "        # Initialization\n",
    "        self.init = init\n",
    "        if self.init: init_cnn(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('x', x.shape)\n",
    "        x = self.input_dropout(x)\n",
    "        x = self.block1(x)\n",
    "        print('x', x.shape)\n",
    "        x = self.block2(x)\n",
    "        print('x', x.shape)\n",
    "        x = self.block3(x)\n",
    "        print('x', x.shape)\n",
    "        b, c, _ = x.size()\n",
    "        x = self.gap(x).view(b, c)\n",
    "        print('x', x.shape)\n",
    "        x = self.fc_dropout(x)\n",
    "        print('x', x.shape)\n",
    "        x = self.fc1(x)\n",
    "        print('x', x.shape)\n",
    "        if self.y_range:\n",
    "            x = F.sigmoid(x)\n",
    "            x = x * (self.y_range[1] - self.y_range[0])\n",
    "            x = x + self.y_range[0]\n",
    "        print('x', x.shape)\n",
    "        return x\n",
    "\n",
    "from functools import partial\n",
    "resnetplus = partial(\n",
    "        ResNetPlus,\n",
    "        input_dropout=0.0,\n",
    "        fc_dropout=0.,\n",
    "        se=False,\n",
    "        reduction=16,\n",
    "        coordconv=False,\n",
    "        y_range=None,\n",
    "        init=False)\n",
    "setattr(resnetplus, '__name__', 'resnetplus')\n",
    "setattr(resnetplus, 'fn', ResNetPlus)\n",
    "\n",
    "\n",
    "resnetplus(c_in, c_out)(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T18:01:30.579855Z",
     "start_time": "2019-07-29T18:01:30.319213Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-v1",
   "language": "python",
   "name": "fastai-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/xresnet1d.ipynb

import torch.nn as nn
import torch,math,sys
import torch.utils.model_zoo as model_zoo
from functools import partial
from fastai.torch_core import Module
import math

__all__ = ['XResNet_1d', 'xresnet18_1d', 'xresnet34_1d', 'xresnet50_1d', 'xresnet101_1d', 'xresnet152_1d']

# or: ELU+init (a=0.54; gain=1.55)
act_fn = nn.ReLU(inplace=True)

class Flatten(Module):
    def forward(self, x): return x.view(x.size(0), -1)

def init_cnn(m):
    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)
    if isinstance(m, (nn.Conv1d,nn.Linear)): nn.init.kaiming_normal_(m.weight)
    for l in m.children(): init_cnn(l)

def same_padding1d(seq_len, ks, stride=1, dilation=1):
    pad = (stride * (seq_len - 1) - seq_len + ks + (ks - 1) * (dilation - 1)) / 2
    padding = (math.ceil(pad), int(pad))
    return padding

def conv(ni, nf, seq_len, ks=3, stride=1, padding='same', bias=False):
    if padding == 'same': padding = same_padding1d(seq_len, ks, stride=stride)
    else: padding = ks//2
    return nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=padding, bias=bias)

def noop(x): return x

def conv_layer(ni, nf, seq_len, ks=3, stride=1, padding='same', zero_bn=False, act=True):
    bn = nn.BatchNorm1d(nf)
    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)
    layers = [conv(ni, nf, seq_len, ks, stride=stride, padding=padding), bn]
    if act:
        layers.append(act_fn)
    return nn.Sequential(*layers)

class ResBlock(Module):
    def __init__(self, expansion, ni, nh, seq_len, padding='same', stride=1):
        nf,ni = nh*expansion,ni*expansion
        if expansion == 1:
            self.conv_layer1 = conv_layer(ni, nh, seq_len, 3, stride=stride, padding=padding)
            self.conv_layer2 = conv_layer(nh, nf, seq_len, 3, zero_bn=True, act=False, padding=padding)
            self.conv_layer3 = noop
        else:
            self.conv_layer1 = conv_layer(ni, nh, seq_len, 1, padding=padding)
            self.conv_layer2 = conv_layer(nh, nh, seq_len, 3, stride=stride, padding=padding)
            self.conv_layer3 = conv_layer(nh, nf, seq_len, 1, zero_bn=True, act=False, padding=padding)
        # TODO: check whether act=True works better
        self.idconv = noop if ni==nf else conv_layer(ni, nf, seq_len, 1, act=False, padding=padding)
        self.pool = noop if stride==1 else nn.AvgPool1d(2, ceil_mode=True)

    def forward(self, x):
        print('x:', x.shape)
        out = self.conv_layer1(x)
        print('out1:', out.shape)
        out = self.conv_layer2(x)
        print('out2:', out.shape)
        out = self.conv_layer3(x)
        print('out3:', out.shape)

        convid = self.idconv(self.pool(x))
        print('convid:', convid.shape)
        output = act_fn(out + convid)
        print('output:', output.shape)
        return output

class XResNet_1d(nn.Sequential):
    def __init__(self, expansion, layers, c_in, seq_len, c_out=1000, padding='same'):
        stem = []
        sizes = [c_in,32,32,64]
        for i in range(3):
            stem.append(conv_layer(sizes[i], sizes[i+1], seq_len, padding=padding, stride=2 if i==0 else 1))

        block_szs = [64//expansion,64,128,256,512]
        blocks = [self._make_layer(expansion, block_szs[i], block_szs[i+1], l, 1 if i==0 else 2, padding=padding)
                  for i,l in enumerate(layers)]
        super().__init__(
            *stem,
            nn.MaxPool1d(kernel_size=3, stride=2, padding=1),
            *blocks,
            nn.AdaptiveAvgPool1d(1), Flatten(),
            nn.Linear(block_szs[-1]*expansion, c_out),
        )
        init_cnn(self)

    def _make_layer(self, expansion, ni, nf, blocks, stride, padding='same'):
        return nn.Sequential(
            *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1, padding=padding)
              for i in range(blocks)])

def xresnet(expansion, n_layers, name, pretrained=False, **kwargs):
    model = XResNet_1d(expansion, n_layers, **kwargs)
    if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[name]))
    return model

me = sys.modules[__name__]
for n, e, l in [
    [18, 1, [2, 2, 2, 2]],
    [34, 1, [3, 4, 6, 3]],
    [50, 4, [3, 4, 6, 3]],
    [101, 4, [3, 4, 23, 3]],
    [152, 4, [3, 8, 36, 3]],
]:
    name = f'xresnet{n}_1d'
    setattr(me, name, partial(xresnet, expansion=e, n_layers=l, name=name))